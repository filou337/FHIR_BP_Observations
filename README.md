Ci-dessous, tu as **un README.md complet** (copie/colle tel quel) **inspirÃ© du style** de ton exemple â€œarbres plantÃ©sâ€¦â€, mais adaptÃ© Ã  ton projet **FHIR Blood Pressure â€“ Kafka â†’ Elasticsearch â†’ Kibana**.

> âœ… Je mets le README dans **un seul bloc Markdown**.
> ğŸ” Ã€ la fin (hors README), je te laisse les **sources** que jâ€™ai utilisÃ©es pour les bonnes pratiques + docs officielles.

````markdown
# ğŸ©º SystÃ¨me de surveillance de la pression artÃ©rielle (FHIR) â€“ Kafka, Elasticsearch & Kibana

Ce dÃ©pÃ´t prÃ©sente un **systÃ¨me de streaming temps rÃ©el** qui gÃ©nÃ¨re des mesures de **pression artÃ©rielle** au format **FHIR Observation (JSON)**, les publie dans **Kafka**, dÃ©tecte et catÃ©gorise les anomalies via un **consumer Python**, puis indexe les **cas anormaux** dans **Elasticsearch** pour une visualisation dans **Kibana**.

âœ… Les mesures **NORMAL** sont **archivÃ©es localement** (JSONL) pour constituer un dataset exploitable (analyses + option ML).

---

## ğŸ¯ Objectifs du projet

- GÃ©nÃ©rer des Observations **FHIR Blood Pressure** (SYS/DIA) rÃ©alistes cÃ´tÃ© backend Python.
- Mettre en place un pipeline **streaming** :
  - **Producer** â†’ Kafka (topic `fhir-observations-raw`)
  - **Consumer** â†’ validation + enrichissement + catÃ©gorisation + routage
  - **Elasticsearch** â†’ stockage des **anomalies uniquement**
  - **Kibana** â†’ visualisation & suivi
- Appliquer des **rÃ¨gles cliniques** (AHA + hypotension) pour classer les mesures :
  - `NORMAL`, `ELEVATED`, `HYPERTENSION_STAGE_1`, `HYPERTENSION_STAGE_2`, `HYPERTENSIVE_CRISIS`, `HYPOTENSION`
- (Optionnel) Ajouter une **brique Machine Learning** (proba de risque) pour complÃ©ter les seuils.

---

## ğŸ§± Architecture (vue dâ€™ensemble)

**Flux :**
1. `fhir_producer.py` gÃ©nÃ¨re des Observations FHIR BP â†’ publie dans Kafka (`fhir-observations-raw`)
2. `fhir_consumer.py` consomme â†’ valide â†’ enrichit â†’ catÃ©gorise â†’ route :
   - `fhir-observations-validated` (toutes les observations enrichies)
   - `blood-pressure-alerts` (uniquement alertes MEDIUM/HIGH/CRITICAL)
   - `error-messages` (messages invalides / erreurs de parsing)
   - `monitoring-metrics` (mÃ©triques pÃ©riodiques)
3. Elasticsearch indexe **uniquement les anomalies** (â‰  `NORMAL`) dans un index **journalier**
4. Kibana permet la **visualisation** + tableaux de bord

> ğŸ“Œ Ajoute ici ton schÃ©ma dâ€™architecture :
- `docs/architecture.png`

---

## ğŸ—‚ Contenu du dÃ©pÃ´t

### Scripts Python
- `fhir_data_generator.py`  
  GÃ©nÃ¨re des donnÃ©es rÃ©alistes et construit une **FHIR Observation Blood Pressure**.

- `fhir_producer.py`  
  Producer Kafka :  
  - crÃ©e les topics si absents (idempotent)
  - publie les Observations sur `fhir-observations-raw`
  - publie aussi `monitoring-metrics` + `audit-log`

- `fhir_consumer.py`  
  Consumer Kafka + pipeline :
  - parsing/validation
  - enrichissement (patient/practitioner, stress, trend, risk scoreâ€¦)
  - catÃ©gorisation BP (AHA + hypotension)
  - indexation ES **si anomalie**
  - archivage local JSONL des NORMAL (`archives/`)

### Infra Docker
- `Docker-compose.yml`  
  Lance :
  - Zookeeper
  - Kafka (Confluent)
  - Elasticsearch 8.11.1
  - Kibana 8.11.1
  - Kafka UI

> Tu peux ajouter plus tard :
- `docs/` (captures Kibana / Kafka UI, schÃ©mas)
- `dashboards/` (exports Kibana)
- `requirements.txt` (dÃ©pendances exactes)
- `.env.example` (variables dâ€™environnement)

---

## ğŸ“¦ Stack & Services

### Docker
- Zookeeper
- Kafka (Confluent CP)
- Elasticsearch `8.11.1`
- Kibana `8.11.1`
- Kafka UI

### Python (librairies)
- `confluent-kafka`
- `elasticsearch`
- `faker`
- `numpy`
- `fhir.resources`

---

## ğŸš€ DÃ©marrage rapide

### 1) Lancer lâ€™infrastructure Docker

> Le fichier sâ€™appelle **`Docker-compose.yml`** (D majuscule).

```bash
docker compose -f Docker-compose.yml up -d
````

### 2) AccÃ©der aux interfaces

* **Kafka UI** : [http://localhost:8080](http://localhost:8080)
* **Kibana** : [http://localhost:5601](http://localhost:5601)
* **Elasticsearch** : [http://localhost:9200](http://localhost:9200)
* **Kafka (host)** : `localhost:9092`

---

## ğŸ Setup Python (venv recommandÃ©)

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS/Linux
source .venv/bin/activate
```

Installer les dÃ©pendances :

```bash
pip install -U pip
pip install confluent-kafka elasticsearch faker numpy fhir.resources
```

---

## â–¶ï¸ ExÃ©cution (ordre conseillÃ©)

### 1) Lancer le consumer (dâ€™abord)

```bash
python fhir_consumer.py
```

âœ… Le consumer :

* crÃ©e un **index template** Elasticsearch `fhir-observations-template`
* consomme `fhir-observations-raw`
* publie `validated`, `alerts`, `metrics`, `errors`
* indexe **uniquement** les anomalies dans `fhir-observations-YYYY.MM.DD`
* archive les NORMAL dans `archives/normal_observations_YYYY-MM-DD.jsonl`

### 2) Lancer le producer

```bash
python fhir_producer.py
```

âœ… Le producer :

* initialise patients / practitioners
* envoie des Observations toutes les **1 Ã  5 secondes**
* garde lâ€™ordre par patient via `key = patient_id`

---

## âš™ï¸ Configuration

### Kafka bootstrap (IMPORTANT)

Dans tes scripts :

* `KAFKA_BOOTSTRAP = "localhost:9092"`

ğŸ‘‰ Si tu exÃ©cutes les scripts **dans Docker**, utilise :

* `kafka:29092`

### Elasticsearch hosts

* Par dÃ©faut : `ES_HOSTS = ["http://localhost:9200"]`
* Dans Docker : `http://elasticsearch:9200`

---

## ğŸ“¨ Topics Kafka

| Topic                         | Description                               | Produit par         |
| ----------------------------- | ----------------------------------------- | ------------------- |
| `fhir-observations-raw`       | Observations FHIR brutes (Blood Pressure) | Producer            |
| `fhir-observations-validated` | Observations enrichies + validÃ©es         | Consumer            |
| `blood-pressure-alerts`       | Alertes uniquement (MEDIUM/HIGH/CRITICAL) | Consumer            |
| `error-messages`              | Erreurs parsing/validation/processing     | Producer + Consumer |
| `monitoring-metrics`          | MÃ©triques pÃ©riodiques (throughput, stats) | Producer + Consumer |
| `audit-log`                   | TraÃ§abilitÃ© minimale (start, etc.)        | Producer            |
| `ml-features` (opt)           | Features prÃªtes ML                        | (Ã  brancher)        |
| `ml-predictions` (opt)        | PrÃ©dictions ML temps rÃ©el                 | (Ã  brancher)        |

---

## ğŸ§¾ Structure des donnÃ©es (FHIR Observation Blood Pressure)

Chaque message est une ressource **FHIR `Observation`** (JSON), de type â€œBlood Pressureâ€, contenant :

* un code panel â€œblood pressureâ€
* deux composants :

  * **Systolic** (`8480-6`)
  * **Diastolic** (`8462-4`)
* unitÃ© : `mm[Hg]`

### Champs extraits / enrichis pour Kibana (exemples)

| Champ                     | Type    | Description                   |
| ------------------------- | ------- | ----------------------------- |
| `patient_id`              | keyword | Identifiant patient           |
| `patient_age`             | integer | Ã‚ge                           |
| `patient_gender`          | keyword | Sexe                          |
| `systolic_pressure`       | integer | SYS                           |
| `diastolic_pressure`      | integer | DIA                           |
| `stress_level`            | integer | Stress simulÃ© (1â€“10)          |
| `blood_pressure_category` | keyword | CatÃ©gorie clinique            |
| `alert_level`             | keyword | NONE/LOW/MEDIUM/HIGH/CRITICAL |
| `risk_score`              | float   | Score de risque               |
| `trend_indicator`         | keyword | Indicateur de tendance        |
| `ingestion_timestamp`     | date    | Timestamp ingestion           |

> Le consumer stocke aussi `observation_full` (FHIR brut) mais avec un mapping ES â€œsafeâ€ (objet dÃ©sactivÃ©) pour Ã©viter une explosion de mapping.

---

## ğŸ§  RÃ¨gles de catÃ©gorisation (AHA + hypotension)

Le consumer applique :

* **Hypotension** : `< 90/60`
* **Normal** : `<120` et `<80`
* **Elevated** : `120â€“129` et `<80`
* **HTA Stage 1** : `130â€“139` ou `80â€“89`
* **HTA Stage 2** : `â‰¥140` ou `â‰¥90`
* **Crise hypertensive** : `>180` et/ou `>120`

---

## ğŸ” Elasticsearch & Kibana

### Indexation Elasticsearch

* Index **journalier** : `fhir-observations-YYYY.MM.DD`
* **Indexation uniquement si anomalie** (â‰  `NORMAL`)
* Template : `fhir-observations-template`

### Kibana : mise en place rapide

1. Ouvre Kibana : [http://localhost:5601](http://localhost:5601)
2. CrÃ©e une **Data View** :

   * Pattern : `fhir-observations-*`
   * Time field : `ingestion_timestamp`

### Dashboards recommandÃ©s (Ã  construire)

* **Vue globale**

  * # anomalies / jour
  * rÃ©partition par catÃ©gorie
  * alertes CRITICAL/HIGH en temps rÃ©el
* **Analyses**

  * SYS/DIA dans le temps (line chart)
  * distribution SYS/DIA (histogram)
  * stress vs SYS/DIA (scatter)
  * top patients par alertes (bar)
* **Filtres**

  * pÃ©riode
  * catÃ©gorie
  * alert_level
  * patient_id / age_group / gender

> ğŸ“Œ Ajoute des captures :

* `docs/kibana-dashboard.png`
* `docs/kafka-ui-topics.png`

---

## ğŸ¤– Section optionnelle : IntÃ©gration dâ€™un modÃ¨le de Machine Learning (bonus)

Pour aller plus loin, tu peux entraÃ®ner un modÃ¨le supervisÃ© (ex : **rÃ©gression logistique**) sur :

* les donnÃ©es NORMAL archivÃ©es en `archives/*.jsonl`
* les anomalies indexÃ©es dans Elasticsearch

Objectif :

* produire une **probabilitÃ© de risque** (au lieu dâ€™un simple seuil)
* publier les sorties sur :

  * `ml-features`
  * `ml-predictions`

âœ… RÃ©sultat : un systÃ¨me plus **prÃ©dictif, probabiliste et adaptable**.

---

## ğŸ§ª VÃ©rifications & Debug

### VÃ©rifier Elasticsearch

```bash
curl http://localhost:9200
curl http://localhost:9200/_cat/indices?v
```

### VÃ©rifier Kibana

```bash
curl http://localhost:5601/api/status
```

### VÃ©rifier Kafka UI

* [http://localhost:8080](http://localhost:8080)
  ContrÃ´le :
* topics prÃ©sents
* messages qui arrivent
* consumer group actif

### ProblÃ¨mes frÃ©quents

* Scripts sur host â†’ `localhost:9092`
* Scripts dans Docker â†’ `kafka:29092`
* Kibana ne dÃ©marre pas â†’ Elasticsearch pas encore â€œhealthyâ€ (attends 30â€“60s)

---

## ğŸ“Œ Roadmap (idÃ©es dâ€™amÃ©lioration)

* [ ] Ajouter un `requirements.txt` (freeze)
* [ ] Dockeriser producer/consumer dans le compose
* [ ] Ajouter exports Kibana dans `dashboards/`
* [ ] Ajouter un script `train_ml.py` (baseline)
* [ ] Ajouter CI (lint + tests)
* [ ] Ajouter un Makefile (start/stop/reset/logs)

---

## ğŸ¤ Contribuer

1. Fork le repo
2. CrÃ©e une branche :

```bash
git checkout -b feature/my-feature
```

3. Commit :

```bash
git commit -m "feat: add my feature"
```

4. Push + Pull Request

---

## ğŸ›¡ï¸ Licence

MIT (recommandÃ©e pour un projet dÃ©mo).
Ajoute un fichier `LICENSE` si besoin.

---

## ğŸ‘¤ Auteur

Philippe ROUMBO

* GitHub : (Ã  complÃ©ter)
* LinkedIn : (Ã  complÃ©ter)

```

