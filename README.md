
# ğŸ©º SystÃ¨me de surveillance FHIR â€” Pression artÃ©rielle (Kafka â€¢ Elasticsearch â€¢ Kibana)

Projet de streaming **temps rÃ©el** autour dâ€™observations de **pression artÃ©rielle** au format **FHIR (JSON)**.
Le systÃ¨me simule des patients (avec `Faker`), publie les mesures dans **Kafka**, applique des **rÃ¨gles de dÃ©tection** (normal / anormal + niveaux dâ€™alerte), puis :
- **archive en local** les mesures normales,
- **indexe dans Elasticsearch** les mesures anormales,
- **visualise dans Kibana** via un tableau de bord.

> RÃ©fÃ©rence dâ€™observation FHIR â€œBlood Pressureâ€ (exemple) :
```txt
https://build.fhir.org/observation-example-bloodpressure.json.html
````

---

## ğŸ¯ Objectifs pÃ©dagogiques

* GÃ©nÃ©rer des observations FHIR rÃ©alistes (patients + praticiens)
* Produire des mesures Ã  une frÃ©quence **1 Ã  5 secondes**
* Mettre en place un **pipeline streaming** (Producer â†’ Kafka â†’ Consumer)
* Appliquer des rÃ¨gles de classification tension artÃ©rielle et dÃ©clencher des alertes
* Stocker et visualiser les anomalies dans **Elasticsearch/Kibana**
* (Optionnel) Ajouter une brique **Machine Learning** pour complÃ©ter les rÃ¨gles (score/proba de risque)

---

## ğŸ§± Architecture (vue dâ€™ensemble)

```txt
[Python - GÃ©nÃ©rateur FHIR] 
        â†“
[Kafka topic: fhir-observations-raw]
        â†“
[Python - Consumer (validation + enrichissement + rÃ¨gles)]
        â”œâ”€ NORMAL  â†’ archives/ (JSONL local)
        â””â”€ ANORMAL â†’ Elasticsearch (index journalier) + alertes Kafka
                         â†“
                      Kibana
```

---

## ğŸ§° Stack technique

* Python 3.11+
* Kafka (via Docker Compose)
* Elasticsearch + Kibana (via Docker Compose)
* `fhir.resources`, `Faker`, `confluent-kafka`, `elasticsearch`, `pandas`, `numpy`
* (Bonus ML) `scikit-learn` + `xgboost`

---

## ğŸ“š DonnÃ©es FHIR utilisÃ©es (minimal)

On conserve uniquement les champs nÃ©cessaires Ã  lâ€™analyse :

* **Patient**
* **Practitioner**
* **Systolic**
* **Diastolic**

---

## ğŸ§µ Topics Kafka

### EntrÃ©e

* `fhir-observations-raw` : observations FHIR brutes (Blood Pressure)

### Sorties (pipeline)

* `fhir-observations-validated` : observation validÃ©e + enrichie
* `blood-pressure-alerts` : alertes (quand la TA est anormale, selon le niveau)
* `error-messages` : erreurs de parsing / validation
* `monitoring-metrics` : mÃ©triques de fonctionnement (dÃ©bit, volumes, etc.)
* `audit-log` : Ã©vÃ©nements de traÃ§abilitÃ© (dÃ©marrage, etc.)

### (Optionnel ML)

* `ml-features` : features envoyÃ©es vers un pipeline ML (si activÃ©)
* `ml-predictions` : prÃ©dictions ML (si activÃ©)

---

## âœ… RÃ¨gles de routage (logique mÃ©tier)

Ã€ chaque observation consommÃ©e :

1. **Validation minimale** du message
2. **Enrichissement** (patient/praticien + champs utiles + scoring)
3. Classification tension :

* **NORMAL** â†’ archivage local en `archives/*.jsonl`
* **NOT NORMAL** â†’ indexation dans Elasticsearch + exposition Kibana + alertes Kafka

---

## ğŸš€ Installation & dÃ©marrage

### 1) PrÃ©requis

* Docker Desktop (ou Docker + Compose)
* Python 3.11+
* Git

### 2) Lancer lâ€™infrastructure (Kafka + ES + Kibana + Kafka-UI)

```bash
docker compose -f Docker-compose.yml up -d
```

### 3) Installer les dÃ©pendances Python

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate

pip install -r Requierement.txt
```

---

## â–¶ï¸ ExÃ©cution du pipeline temps rÃ©el

> Ouvre 2 terminaux (venv activÃ© dans les deux).

### Terminal A â€” Consumer (analyse + routage)

```bash
python fhir_consumer.py
```

### Terminal B â€” Producer (gÃ©nÃ©ration + publication)

```bash
python fhir_producer.py
```

---

## ğŸŒ Interfaces utiles

* **Kibana** : [http://localhost:5601](http://localhost:5601)
* **Elasticsearch** : [http://localhost:9200](http://localhost:9200)
* **Kafka UI** : [http://localhost:8080](http://localhost:8080)

---

## ğŸ“Š Visualisation Kibana (idÃ©e de dashboard)

Dans Kibana, tu peux crÃ©er :

* Un tableau des derniÃ¨res alertes (par patient / catÃ©gorie / niveau)
* Une courbe â€œsystolic/diastolicâ€ dans le temps
* Un filtre par :

  * catÃ©gorie (`NORMAL`, `ELEVATED`, `STAGE_1`, `STAGE_2`, `CRISIS`)
  * niveau dâ€™alerte
  * patient / praticien

---

## ğŸ¤– Section optionnelle â€” IntÃ©gration dâ€™un modÃ¨le Machine Learning

Objectif : complÃ©ter les rÃ¨gles par seuils avec une couche â€œintelligenteâ€ :

* **prÃ©diction** du niveau de risque / catÃ©gorie en temps rÃ©el,
* **estimation probabiliste** (ex : proba dâ€™Ãªtre en `STAGE_2`),
* meilleur rÃ©alisme et adaptabilitÃ© (bruit, tendances, profils patients, etc.).

### 1) PrÃ©parer un dataset de features (CSV)

Le pipeline ML attend un fichier du type :

* `ml_data/blood_pressure_features.csv`

Features typiques :

* `systolic`, `diastolic`, `age`, `gender`, `trend`, `risk_score`, `hour_of_day`
  Target :
* `blood_pressure_category` (5 classes)

### 2) EntraÃ®ner le modÃ¨le

```bash
python ml_training.py ml_data/blood_pressure_features.csv
```

RÃ©sultat attendu :

* un modÃ¨le sauvegardÃ© dans `ml_models/` (modÃ¨le + scaler + metadata)

### 3) Brancher le modÃ¨le dans le streaming (concept)

Deux options propres :

* **Option A (simple)** : le consumer charge le modÃ¨le au dÃ©marrage et ajoute `ml_prediction`/`ml_proba` au document indexÃ© ES.
* **Option B (streaming ML)** : le consumer publie des `ml-features` â†’ un service ML renvoie `ml-predictions`.

---

## ğŸ—‚ï¸ Structure recommandÃ©e du projet

```txt
.
â”œâ”€â”€ Docker-compose.yml
â”œâ”€â”€ Requierement.txt
â”œâ”€â”€ fhir_data_generator.py
â”œâ”€â”€ fhir_producer.py
â”œâ”€â”€ fhir_consumer.py
â”œâ”€â”€ ml_training.py
â”œâ”€â”€ ml_feature_extraction.py
â”œâ”€â”€ ml_data/
â”‚   â””â”€â”€ blood_pressure_features.csv
â”œâ”€â”€ ml_models/
â””â”€â”€ archives/
```

---

## ğŸ§¯ DÃ©pannage rapide

* Kafka/ES/Kibana ne rÃ©pondent pas :

```bash
docker compose -f Docker-compose.yml ps
docker compose -f Docker-compose.yml logs -f
```

* Kibana dÃ©marre lentement : attendre 1â€“2 minutes (ES doit Ãªtre â€œhealthyâ€)
* Conflits de ports : vÃ©rifier que 9092 / 9200 / 5601 / 8080 sont libres

---

## ğŸ‘¤ Auteurs

**Philippe ROUMBO**
**Salma ELABSODI**

```

Le README ci-dessus est alignÃ© sur vos scripts (topics, routage NORMALâ†’archives et anomaliesâ†’Elasticsearch, modÃ¨le ML optionnel) et votre infra Docker (Kafka/Zookeeper, Elasticsearch, Kibana, Kafka-UI, ports). :contentReference[oaicite:0]{index=0} :contentReference[oaicite:1]{index=1} :contentReference[oaicite:2]{index=2} :contentReference[oaicite:3]{index=3}
```
